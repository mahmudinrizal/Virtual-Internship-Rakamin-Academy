# -*- coding: utf-8 -*-
"""final project rakamin VIX.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dBhE2vz4-RD7VUllk8QwnHRHvWxNgEhB
"""

import pandas as pd 
import numpy as np 
import matplotlib.pyplot as plt 
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler 
from sklearn.linear_model import LogisticRegressionCV
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.naive_bayes import BernoulliNB
from sklearn import preprocessing
from sklearn.model_selection import KFold,cross_val_score, RepeatedStratifiedKFold,StratifiedKFold, GridSearchCV
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.preprocessing import OneHotEncoder,StandardScaler,PowerTransformer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.impute import SimpleImputer
from sklearn.dummy import DummyClassifier
from imblearn.over_sampling import SMOTE
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score, ConfusionMatrixDisplay,RocCurveDisplay, precision_score, recall_score, f1_score, classification_report, roc_curve, plot_roc_curve, auc, precision_recall_curve, plot_precision_recall_curve, average_precision_score
from imblearn.over_sampling import SMOTE

import plotly 
import plotly.express as px
import plotly.graph_objs as go
import plotly.offline as py
from plotly.offline import iplot
from plotly.subplots import make_subplots
import plotly.figure_factory as ff
import matplotlib.ticker as mtick


import missingno as msno

import warnings
warnings.filterwarnings("ignore")

from google.colab import drive
drive.mount('/content/gdrive')

df = pd.read_csv('/content/gdrive/MyDrive/Colab Notebooks/Rakamin VIX-IDX/loan_data_2007_2014.csv')
df = df.drop("Unnamed: 0", axis=1)
df.head()

"""# **A. DATA CLEANING AND PREPARATION**"""

df.info()

"""**A.1. Handling Missing Values**

Feature with missing values + zero values with total % more than 15% will be droped
"""

def missing_values_table(df):
        zero_val = (df == 0.00).astype(int).sum(axis=0)
        mis_val = df.isnull().sum()
        mis_val_percent = 100 * df.isnull().sum() / len(df)
        mv_table = pd.concat([zero_val, mis_val, mis_val_percent], axis=1)
        mv_table = mv_table.rename(columns = {0 : 'Zero Values', 1 : 'Missing Values', 2 : '% of Missing Values'})
        mv_table['Total Zero + Missing Values'] = mv_table['Zero Values'] + mv_table['Missing Values']
        mv_table['% Total Zero + Missing Values'] = 100 * mv_table['Total Zero + Missing Values'] / len(df)
        mv_table['Data Type'] = df.dtypes
        mv_table = mv_table[
            mv_table.iloc[:,1] != 0].sort_values(
        '% of Missing Values', ascending=False).round(1)
        print ("Your selected dataframe has " + str(df.shape[1]) + " columns and " + str(df.shape[0]) + " Rows.\n"      
            "There are " + str(mv_table.shape[0]) +
              " columns that have missing values.")
        return mv_table

miss_table = missing_values_table(df)
miss_table

# miss_table[miss_table['% Total Zero + Missing Values'] > 15]
miss_table.drop(miss_table[(miss_table['% Total Zero + Missing Values'] < 15)].index, inplace=True)
feature_to_drop = list(miss_table.index.values)

df.drop(columns=feature_to_drop, axis=1, inplace=True)

df.shape

df.isnull().sum().sort_values(ascending=False)

"""**A.2. Remove Meaningless Features**"""

for col in df.columns:
  print('---', col, '---')
  c = df[col].value_counts(dropna=False)
  p = df[col].value_counts(dropna=False, normalize=True)
  print(pd.concat([c,p], axis=1, keys=['counts', '%']))
  print('-----')

meaningless_features = ['id','emp_title','url', 'zip_code', 'title', 'sub_grade', 'policy_code', 'application_type', 'member_id', 'last_pymnt_d', 'last_pymnt_amnt',
       'last_credit_pull_d', 'earliest_cr_line', 'issue_d']

df.drop(columns=meaningless_features, axis=1, inplace=True)

df.shape

df.columns

"""**A.3. Handling Target Features**"""

df.loan_status.value_counts()

df['loan_status'] = df['loan_status'].replace(['Late (31-120 days)', 'Late (16-30 days)', 'Default'], ['Charged Off', 'Charged Off', 'Charged Off'])

df = df.loc[df['loan_status'].isin(['Fully Paid', 'Charged Off'])]

df.loan_status.value_counts()

df.shape

"""# **B. EDA**"""

len(df.loan_status)

df_eda = df.copy()

"""**with y-axis as count**"""

def show_values_on_bars(axs, features):
    total = len(features)
    def _show_on_single_plot(ax):        
        for p in ax.patches:
            _x = p.get_x() + p.get_width() / 2
            _y = p.get_y() + p.get_height()
            value = '{:.2f}%'.format(100 * p.get_height()/total)
            ax.text(_x, _y, value, ha="center", va='bottom', size=12) 

    if isinstance(axs, np.ndarray):
        for idx, ax in np.ndenumerate(axs):
            _show_on_single_plot(ax)
    else:
        _show_on_single_plot(axs)

def with_hue(axs, feature, Number_of_categories, hue_categories):
    def _show_on_single_plot2(ax): 
        all_heights = [[p.get_height() for p in bars] for bars in ax.containers]
        for bars in ax.containers:
            for i, p in enumerate(bars):
                total = sum(xgroup[i] for xgroup in all_heights)
                percentage = f'{(100 * p.get_height() / total) :.1f}%'
                ax.text(p.get_x() + p.get_width() / 2, p.get_height(), percentage, size=11, ha='center', va='bottom', rotation=45)

    if isinstance(axs, np.ndarray):
          for idx, ax in np.ndenumerate(axs):
              _show_on_single_plot2(ax)
    else:
          _show_on_single_plot2(axs)

f, axes = plt.subplots(1, 2, figsize=(20,5))
sns.countplot(x='loan_status', data=df, ax=axes[0])
show_values_on_bars(axes, df.loan_status)

sns.countplot(x='grade', hue='loan_status', data=df, order=sorted(df['grade'].unique()), ax=axes[1])
with_hue(axes[1], df.grade, 7, 2)

sns.despine()
axes[0].set(xlabel='Status', ylabel='')
axes[0].set_title('% of Loan Status', size=20, pad=30)
axes[1].set(xlabel='Grade', ylabel='')
axes[1].set_title('% of Loan Status by grade', size=20, pad=30)

df['emp_length'] = df['emp_length'].replace(['< 1 year'],['less than 1 y'])

df['pymnt_plan'].value_counts()

f, axes = plt.subplots(1, 2, figsize=(20,5))
sns.countplot(x='term', hue='loan_status', data=df, order=sorted(df['term'].unique()), ax=axes[0])
with_hue(axes[0], df.emp_length, 2, 2)

sns.countplot(x='emp_length', hue='loan_status', data=df, ax=axes[1])
with_hue(axes[1], df.emp_length, 11, 2)

sns.despine()
axes[0].set(xlabel='Term', ylabel='')
axes[0].set_title('% of Loan Status by term', size=20, pad=30)
axes[1].set(xlabel='Emp Length', ylabel='')
axes[1].set_title('% of Loan Status by emp length', size=20, pad=30)
axes[1].tick_params(axis='x', labelrotation = 45)

f, axes = plt.subplots(1, 2, figsize=(20,5))
sns.countplot(x='purpose', data=df, order=df['purpose'].value_counts().index, ax=axes[0])
show_values_on_bars(axes[0], df.loan_status)

sns.scatterplot(x='installment', y='loan_amnt', data=df, ax=axes[1])

sns.despine()
axes[0].set(xlabel='Purpose', ylabel='')
axes[0].set_title('Count of Loan Purpose', size=20, pad=30)
axes[0].tick_params(axis='x', labelrotation = 90)
axes[1].set(xlabel='Emp Length', ylabel='')
axes[1].set_title('Distribution between Loant Amount and Installment', size=20, pad=30)

"""**with y-axis as percentage**"""

f, axes = plt.subplots(1, 2, figsize=(20,5))

df_eda['percentage(%)'] = 0 # a dummy column to refer to
for col, ax in zip(['grade', 'pymnt_plan'], axes.flatten()):
    counts = df_eda.groupby([col, 'loan_status']).count()
    freq_per_group = counts.div(counts.groupby(col).transform('sum')).reset_index()
    sns.barplot(x=col, y='percentage(%)', hue='loan_status', data=freq_per_group, ax=ax)

with_hue(axes[0], df_eda.grade, 7, 2)
with_hue(axes[1], df_eda.pymnt_plan, 2, 2)

sns.despine()
axes[0].set(xlabel='Grade', ylabel='Percentage (%)')
axes[0].set_title('% of Loan Status by Grade', size=20, pad=30)
axes[1].set(xlabel='Payment Plan', ylabel='Percentage (%)')
axes[1].set_title('% of Loan Status by Payment Plan', size=20, pad=30)
axes[0].yaxis.set_major_formatter(mtick.PercentFormatter(xmax=1))
axes[1].yaxis.set_major_formatter(mtick.PercentFormatter(xmax=1))

f, axes = plt.subplots(1, 2, figsize=(20,5))

df_eda['percentage(%)'] = 0 # a dummy column to refer to
for col, ax in zip(['term', 'emp_length'], axes.flatten()):
    counts = df_eda.groupby([col, 'loan_status']).count()
    freq_per_group = counts.div(counts.groupby(col).transform('sum')).reset_index()
    sns.barplot(x=col, y='percentage(%)', hue='loan_status', data=freq_per_group, ax=ax)

with_hue(axes[0], df_eda.grade, 7, 2)
with_hue(axes[1], df_eda.pymnt_plan, 2, 2)

sns.despine()
axes[0].set(xlabel='Term', ylabel='Percentage (%)')
axes[0].set_title('% of Loan Status by Term', size=20, pad=30)
axes[1].set(xlabel='Emp Length', ylabel='Percentage (%)')
axes[1].set_title('% of Loan Status by Emp Length', size=20, pad=30)
axes[0].yaxis.set_major_formatter(mtick.PercentFormatter(xmax=1))
axes[1].yaxis.set_major_formatter(mtick.PercentFormatter(xmax=1))

f, axes = plt.subplots(1, 2, figsize=(20,5))

sns.histplot(df_eda, x="int_rate", hue="loan_status", element="poly", ax=axes[0])
sns.histplot(df_eda, x="loan_amnt", hue="loan_status", element="poly", ax=axes[1])

sns.despine()
axes[0].set(xlabel='int rate', ylabel='Percentage (%)')
axes[0].set_title('% of Loan Status by int rate', size=20, pad=30)
axes[1].set(xlabel='loan amount', ylabel='Percentage (%)')
axes[1].set_title('% of Loan Status by loan amount', size=20, pad=30)

"""
# **C. Features Engineering**"""

df_model = df.copy()

"""**C. 1. Handling Categorical Features**"""

categorical_col = list(df_model.select_dtypes(include=["object"]).columns)
categorical_col

"""Fill missing value with mode"""

df_model[categorical_col].isnull().sum().sort_values(ascending=False)

df_model['emp_length'] = df_model['emp_length'].fillna(df_model['emp_length'].mode()[0])

df_model[categorical_col].isnull().sum().sort_values(ascending=False)

"""Encoding Features


*   One-Hot Encoding
*   Label Encoding
"""

label_encoder = preprocessing.LabelEncoder()

label_enc_features = ['grade', 'emp_length', 'home_ownership']
for col in label_enc_features:
    df_model[col]= label_encoder.fit_transform(df_model[col])

df_onehot = pd.get_dummies(df_model[['term']], prefix='term', prefix_sep="_")
df_onehot2 = pd.get_dummies(df_model[['verification_status']], prefix="verification_status", prefix_sep="_")
df_onehot3 = pd.get_dummies(df_model[['pymnt_plan']], prefix="pymnt_plan", prefix_sep="_")
df_onehot4 = pd.get_dummies(df_model[['purpose']], prefix="purpose", prefix_sep="_")
df_onehot5 = pd.get_dummies(df_model[['addr_state']], prefix="addr_state", prefix_sep="_")
df_onehot6 = pd.get_dummies(df_model[['initial_list_status']], prefix="initial_list_status", prefix_sep="_")

df_final_onehot = df_onehot.join([df_onehot2, df_onehot3, df_onehot4, df_onehot5, df_onehot6])

"""**C. 2. Handling Numerical Features**"""

numerical_col = list(df_model.select_dtypes(exclude=["object"]).columns)
numerical_col

"""Filling Missing value with median"""

df_model[numerical_col].isnull().sum().sort_values(ascending=False)

median_value = df_model['revol_util'].median()
df_model['revol_util'].fillna(value = median_value, inplace = True)

df_model[numerical_col].isnull().sum()

df_model = df_final_onehot.join(df_model)

df_model.drop(columns=['term', 'verification_status', 'pymnt_plan', 'purpose', 'addr_state', 'initial_list_status'], inplace=True)

"""**C. 3. Handling Target Features**"""

df_model['loan_status'].value_counts()

df_model['loan_status'] = df_model['loan_status'].map({'Fully Paid': 1, 'Charged Off': 0})

df_model['loan_status'].value_counts()

"""# **D. Modelling and Evaluation ML**"""

df_model.columns

x = df_model.drop(['loan_status'],axis=1)
y = df_model['loan_status']

X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 42)

from sklearn.preprocessing import StandardScaler
ss = StandardScaler()

x_train_scaled = ss.fit_transform(X_train)
x_test_scaled = ss.transform(X_test)

sm = SMOTE(random_state=2)
x_train_res, y_train_res = sm.fit_resample(x_train_scaled, y_train.ravel())

unique, counts = np.unique(y_train_res, return_counts=True)
dict(zip(unique, counts))

models = pd.DataFrame(columns=['Model', 'Precision', 'Recall', 'Accuracy', 'F1-Score'])

"""logistic Reg"""

from sklearn.linear_model import LogisticRegression
log_reg_model = LogisticRegression(max_iter=1000) #model
log_reg_model.fit(x_train_res,y_train_res) 

train_predictions_log = log_reg_model.predict(x_train_res) #train the model
test_predictions_log = log_reg_model.predict(x_test_scaled) #test the model

precision = precision_score(y_test,test_predictions_log)
recall = recall_score(y_test,test_predictions_log)
accu = accuracy_score(y_test,test_predictions_log)
f1_sc = f1_score(y_test,test_predictions_log) 

print('Logistic regression model evaluation:')
print('Precision: ', precision)
print('Recall: ', recall)
print('\nAccuracy: ',accu)
print('f1_score: ', f1_sc)

new_row = {"Model": "Logistic Regression","Precision": precision, "Recall": recall, "Accuracy": accu, "F1-Score": f1_sc}
models = models.append(new_row, ignore_index=True)

from sklearn.metrics import plot_confusion_matrix
plot_confusion_matrix(log_reg_model,x_test_scaled,y_test)

from sklearn.metrics import plot_confusion_matrix, classification_report

print(classification_report(y_test,test_predictions_log))

print('Accuracy Score: ',accuracy_score(y_test,test_predictions_log))

"""Random Forest"""

from sklearn.ensemble import RandomForestClassifier
random_forest_model = RandomForestClassifier(random_state=42) #model
random_forest_model.fit(x_train_res,y_train_res) 

train_predictions_rf = random_forest_model.predict(x_train_res) #train the model
test_predictions_rf = random_forest_model.predict(x_test_scaled) #test the model

precision = precision_score(y_test,test_predictions_rf)
recall = recall_score(y_test,test_predictions_rf)
accu = accuracy_score(y_test,test_predictions_rf)
f1_sc = f1_score(y_test,test_predictions_rf) 

print('Random Forest Classifier model evaluation:')
print('Precision: ', precision)
print('Recall: ', recall)
print('\nAccuracy: ',accu)
print('f1_score: ', f1_sc)

new_row = {"Model": "Random Forest Classifier","Precision": precision, "Recall": recall, "Accuracy": accu, "F1-Score": f1_sc}
models = models.append(new_row, ignore_index=True)

plot_confusion_matrix(random_forest_model,x_test_scaled,y_test)

print(classification_report(y_test,test_predictions_rf))

print('Accuracy Score: ',accuracy_score(y_test,test_predictions_rf))

"""Decission Tree"""

from sklearn.tree import DecisionTreeClassifier
dtc_model = DecisionTreeClassifier() #model
dtc_model.fit(x_train_scaled,y_train) 

train_predictions_dtc = dtc_model.predict(x_train_scaled) #train the model
test_predictions_dtc = dtc_model.predict(x_test_scaled) #test the model

precision = precision_score(y_test,test_predictions_dtc)
recall = recall_score(y_test,test_predictions_dtc)
accu = accuracy_score(y_test,test_predictions_dtc)
f1_sc = f1_score(y_test,test_predictions_dtc) 

print('Decision Tree Classifier model evaluation:')
print('Precision: ', precision)
print('Recall: ', recall)
print('\nAccuracy: ',accu)
print('f1_score: ', f1_sc)

new_row = {"Model": "Decision Tree Classifier","Precision": precision, "Recall": recall, "Accuracy": accu, "F1-Score": f1_sc}
models = models.append(new_row, ignore_index=True)

plot_confusion_matrix(dtc_model,x_test_scaled,y_test)

print(classification_report(y_test,test_predictions_dtc))

print('Accuracy Score: ',accuracy_score(y_test,test_predictions_dtc))

models

from sklearn.metrics import roc_auc_score
from sklearn.metrics import roc_curve
from matplotlib.pyplot import figure

figure(figsize=(8, 6), dpi=80)
fpr, tpr, _ =roc_curve(y_test, test_predictions_log)
auc = roc_auc_score(y_test, test_predictions_log)
plt.plot(fpr,tpr,label="Log Reg="+str(auc),color='darkorange')
plt.plot([0, 1], [0, 1], color="navy", linestyle="--")

fpr, tpr, _ =roc_curve(y_test,  test_predictions_rf)
auc = roc_auc_score(y_test, test_predictions_rf)
plt.plot(fpr,tpr,label="Random Forest="+str(auc),color='darkgreen')
plt.plot([0, 1], [0, 1], color="navy", linestyle="--")

fpr, tpr, _ =roc_curve(y_test,  test_predictions_dtc)
auc = roc_auc_score(y_test, test_predictions_dtc)
plt.plot(fpr,tpr,label="Decission Tree="+str(auc),color='red')
plt.plot([0, 1], [0, 1], color="navy", linestyle="--")

plt.ylabel('True Positive Rate')
plt.xlabel('False Positive Rate')
plt.legend(loc=4)
plt.show()

"""**Features Importance**"""

importances = pd.DataFrame(data={
    'Attribute': X_train.columns,
    'Importance': random_forest_model.feature_importances_
})
importances = importances.head(10).sort_values(by='Importance', ascending=False)

plt.bar(x=importances['Attribute'], height=importances['Importance'], color='#087E8B')
plt.title('Feature importances obtained from coefficients', size=20)
plt.xticks(rotation='vertical')
plt.show()